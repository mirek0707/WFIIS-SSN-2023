{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorium 4 - Recurrent Neural Network cz. II"
      ],
      "metadata": {
        "id": "8k2vE9czjf6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 1 - Wprowadzenie\n",
        "Stwórz model Simple RNN, który będzie przewidywał ceny mieszkań na podstawie danych z [Boston Housing](https://keras.io/api/datasets/boston_housing/). Większość problemów i implementacji powinna już być znana z poprzednich zajęć więć, prześledzimy to rozwiązanie i poszukamy potencjalnych rozwiązań/ulepszeń."
      ],
      "metadata": {
        "id": "_Utmviufb-f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import boston_housing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Dropout,LSTM, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "m4IZN00FdMWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "metadata": {
        "id": "EFJpi3VkdNQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Qi4H8Uq7dQiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=32, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dense(units=1))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7CGv_GFIdSRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "history = model.fit(X_train_scaled[:,:,np.newaxis], y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "xsLNb0YQdUFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled[:,:,np.newaxis])\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(y_pred, label='predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n8WMNPFldV1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 1 - Modyfikacje i usprawnienia\n",
        "Wyniki nie są zadowalające, spróbuj poprawić ten model. Zaproponuj odpowiednie modyfikacje, może LSTM albo GRU zadziałają. Przetestuj i porównaj wyniki. Jakie wnioski się nasuwają?"
      ],
      "metadata": {
        "id": "7P4K5Bw9endl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 2 - Wprowadzenie\n",
        "\n",
        "**Return_sequence=True** to argument funkcji warstwy rekurencyjnej w sieciach neuronowych, który określa, czy dana warstwa powinna zwrócić sekwencję wyników (ang. sequence) dla każdego kroku czasowego, czy tylko wynik dla ostatniego kroku czasowego.\n",
        "\n",
        "W przypadku rekurencyjnych sieci neuronowych (RNN) argument ten jest szczególnie ważny, ponieważ sieci RNN są zdolne do przetwarzania sekwencji danych wejściowych. Każdy krok czasowy w sekwencji jest przetwarzany przez sieć RNN, a wynik w każdym kroku jest używany do wyznaczenia wyniku w następnym kroku.\n",
        "\n",
        "Jeśli **Return_sequence=True**, to warstwa RNN zwróci sekwencję wyników dla każdego kroku czasowego, co oznacza, że wynik na każdym kroku będzie przekazywany do następnego kroku czasowego. To pozwala na dalsze przetwarzanie przez kolejne warstwy sieci lub analizę wyników dla każdego kroku.\n",
        "\n",
        "Jeśli **Return_sequence=False**, to warstwa RNN zwróci tylko wynik ostatniego kroku czasowego, co oznacza, że wynik na każdym kroku jest zignorowany poza wynikiem ostatniego kroku. To jest przydatne w przypadku, gdy sieć RNN jest używana do generowania jednego wyniku na końcu sekwencji, np. w zadaniach przewidywania lub generowania sekwencji.\n",
        "\n",
        "Warto zaznaczyć, że niektóre implementacje sieci RNN pozwalają na ustawienie **Return_sequence=False** dla niektórych warstw i **Return_sequence=True** dla innych, co pozwala na bardziej elastyczne modele sieciowe."
      ],
      "metadata": {
        "id": "EHBPnRKekjHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "model.add(SimpleRNN(units=32, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "model.summary()\n",
        "\n",
        "## TO DO: Wybierz optymalizator(zastanów sie nad parametrami jakie można mu przypisać oraz dobierz funkcje straty!)\n",
        "model.compile(optimizer= ??? , loss='')"
      ],
      "metadata": {
        "id": "Z4bZ17HqdxdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled[:,:,np.newaxis], y_train, epochs=200, batch_size=16, validation_split=0.2)"
      ],
      "metadata": {
        "id": "34-RVkocdzcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled[:,:,np.newaxis])\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(y_pred, label='predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95wV6aP7d12x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 2 - Modyfikacje i usprawnienia\n",
        "Dokonaj modyfikacji, zaproponuj rozwiązania oparte o GRU i LSTM. Jakie wnioski płyną z twojej analizy?"
      ],
      "metadata": {
        "id": "tX2XmbUkmNlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 3 Przetestuj [BLSTM](https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/)."
      ],
      "metadata": {
        "id": "0N7zGu9pcCOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=64, input_shape=(X_train.shape[1], 1), return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "CIXt9puziIyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled[:,:,np.newaxis], y_train, epochs=100, batch_size=16, validation_split=0.2)"
      ],
      "metadata": {
        "id": "BvrAssF6iYb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled[:,:,np.newaxis])\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(y_pred, label='predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F4Jq7GpQiMWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 4 - LSTM, GRU, SimpleRNN w zadaniach z tekstem (zbiór Shakespearea)"
      ],
      "metadata": {
        "id": "CygRAhBAqabh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HdD8tkEUwSlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Wczytanie danych tekstowych\n",
        "with open(path_to_file, 'r') as f:\n",
        "    text = f.read().lower()\n",
        "print('Długość tekstu:', len(text))\n",
        "\n",
        "# Tworzenie słownika znaków\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# Przygotowanie sekwencji danych wejściowych i wyjściowych\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i:i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('Liczba sekwencji:', len(sentences))\n",
        "\n",
        "# Przygotowanie danych wejściowych i wyjściowych\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# Definicja modelu sieci rekurencyjnej\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilacja modelu\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "# Nauczanie modelu\n",
        "model.fit(x, y, batch_size=128, epochs=50)\n",
        "\n",
        "# Generowanie tekstu\n",
        "start_index = np.random.randint(0, len(text) - maxlen - 1)\n",
        "generated_text = text[start_index:start_index + maxlen]\n",
        "print('--- Początek generowanego tekstu: \"' + generated_text + '\"')\n",
        "\n",
        "for i in range(400):\n",
        "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(generated_text):\n",
        "        x_pred[0, t, char_indices[char]] = 1\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = np.random.choice(len(preds), p=preds)\n",
        "    next_char = indices_char[next_index]\n",
        "    generated_text += next_char\n",
        "    generated_text = generated_text[1:]\n",
        "    \n",
        "print('--- Wygenerowany tekst: \"' + generated_text + '\"')\n"
      ],
      "metadata": {
        "id": "oRVxcFZhxd3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 5 - LSTM, GRU, SimpleRNN w zadaniach z tekstem (IMDB, 20groupsnew)\n",
        "Korzystajac z kodu z zajęć poprzednich oraz wiedzy zdobytej na dzisiejszych zaimplementuj lepsze rozwiązanie LSTM,GRU,SimpleRNN z wykorzystaniem return_sequences dla zbioru IMDB. Wykorzystaj przykłady kodu z poprzednich zajęć, bądź przesłanych przez prowadzacego. Może BLSTM?"
      ],
      "metadata": {
        "id": "tlUhsY9jqdP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 6 - sieć seq2seq\n",
        "\n",
        "Z racji że nie mamy wystarczająco czasu na omówienie sieci seq2seq przedstawiam wam przykłady dot. wykorzystania sieci seq2seq w tłumaczeniu tekstów:\n",
        "\n",
        "- [Przykład 1 czesc I](https://blog.paperspace.com/introduction-to-seq2seq-models/)\n",
        "- [Przykład 1 czesc II](https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/)\n",
        "- [Przykład 2](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
        "- [Przykład 3](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)\n",
        "- [Przykład 4](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)\n",
        "- [Przykład 5](https://bgg.medium.com/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad)\n",
        "\n"
      ],
      "metadata": {
        "id": "uaxtwXO1quYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 7 Kompilatory i funkcje straty\n",
        "\n",
        "Zadanie polega na porównaniu osiąganych wyników trzech różnych modeli (SimpleRNN, LSTM, GRU) dla zbioru danych Housing, wykorzystując różne optymalizatory i funkcje straty.\n",
        "\n",
        "* Dla modelu 1 (SimpleRNN):\n",
        "Optymalizator: Adam\n",
        "Funkcja straty: Mean Squared Error\n",
        "\n",
        "* Dla modelu 2 (LSTM):\n",
        "Optymalizator: RMSprop\n",
        "Funkcja straty: Mean Absolute Error\n",
        "* Dla modelu 3 (GRU):\n",
        "Optymalizator: Adagrad\n",
        "Funkcja straty: Mean Squared Logarithmic Error"
      ],
      "metadata": {
        "id": "U9hjzJnXcDbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import boston_housing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Dropout,LSTM, GRU\n",
        "from keras.optimizers import Adam, RMSprop, Adagrad\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(SimpleRNN(units=64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "model_1.add(SimpleRNN(units=32, return_sequences=False))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Dense(units=1))\n",
        "model_1.summary()\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(LSTM(units=64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "model_2.add(LSTM(units=32, return_sequences=False))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(units=1))\n",
        "model_2.summary()\n",
        "\n",
        "model_3 = Sequential()\n",
        "model_3.add(GRU(units=64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
        "model_3.add(GRU(units=32, return_sequences=False))\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Dense(units=1))\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "YI6YS8gLnUyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1\n",
        "model_1.compile(optimizer=Adam(), loss='mse')\n",
        "model_1.fit(X_train_scaled[:,:,np.newaxis], y_train, batch_size=32, epochs=50)"
      ],
      "metadata": {
        "id": "0Xfd5Oahnpcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2\n",
        "model_2.compile(optimizer=RMSprop(), loss='mae')\n",
        "model_2.fit(X_train_scaled[:,:,np.newaxis], y_train, batch_size=32, epochs=50)"
      ],
      "metadata": {
        "id": "BgBJdlZhnsJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3\n",
        "model_3.compile(optimizer=Adagrad(), loss='mean_squared_logarithmic_error')\n",
        "model_3.fit(X_train_scaled[:,:,np.newaxis], y_train, batch_size=32, epochs=50)"
      ],
      "metadata": {
        "id": "sHlsQVD5nxIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_1 = model_1.predict(X_test_scaled[:,:,np.newaxis])\n",
        "y_pred_2 = model_2.predict(X_test_scaled[:,:,np.newaxis])\n",
        "y_pred_3 = model_3.predict(X_test_scaled[:,:,np.newaxis])"
      ],
      "metadata": {
        "id": "HGC3pwjToRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions vs true values\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(y_pred_1, label='SimpleRNN')\n",
        "plt.plot(y_pred_2, label='LSTM')\n",
        "plt.plot(y_pred_3, label='GRU')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "12is5zXRoTbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 8 - Modyfikacje \n",
        "Dokonaj modyfikacji. \n",
        "Wykorzystujac [funkcje straty](https://keras.io/api/losses/) z biblioteki KERAS oraz dostępne tam [optymalizatory](https://keras.io/api/optimizers/) dokonaj porównania:\n",
        "\n",
        "- 3 sieci LSTM\n",
        "- 3 sieci SimpleRNN\n",
        "- 3 sieci GRU\n",
        "\n",
        "dla zbioru [Boston Housing](https://keras.io/api/datasets/boston_housing/)\n",
        "\n",
        "Najistotniejsze w tym zadaniu jest wykorzystanie 3różnych optymalizatorów i funkcji straty dla danej architektury sieci neuronowej i zestawienie wyników. Czy jest zauważalny wpływ na osiągnięte wyniki? Wykorzystaj optymalizatory oraz funkcje straty w sposob **logiczny**."
      ],
      "metadata": {
        "id": "U_-Bxb9qoeni"
      }
    }
  ]
}